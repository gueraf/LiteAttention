{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ac4bffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "KSTAGES = 2\n",
    "\n",
    "def memQ(m, n, k, dtype):\n",
    "    return m * k * dtype.itemsize\n",
    "\n",
    "def memK(m, n, k, dtype):\n",
    "    return n * k * KSTAGES * dtype.itemsize\n",
    "\n",
    "def memV(m, n, k, dtype):\n",
    "    return n * k * KSTAGES * dtype.itemsize\n",
    "\n",
    "def memQK(m, n, k, dtype):\n",
    "    return m * n * dtype.itemsize\n",
    "\n",
    "def memP(m, n, k, dtype):\n",
    "    return m * n * dtype.itemsize\n",
    "\n",
    "def memO(m, n, k, dtype):\n",
    "    return m * k * dtype.itemsize\n",
    "\n",
    "# returns the number of BYTES needed for registers and shared memory\n",
    "def mem_calc(m, n, k, q_dtype = torch.int8, k_dtype = torch.int8, v_dtype = torch.float16, MmaPV_is_RS=False, isIntraWGOverlap=False):\n",
    "    q_size = memQ(m, n, k, q_dtype)\n",
    "    k_size = memK(m, n, k, k_dtype)\n",
    "    v_size = memV(m, n, k, v_dtype)\n",
    "    qk_size = memQK(m, n, k, torch.int32)\n",
    "    p_size = memP(m, n, k, torch.float16)\n",
    "    o_size = memO(m, n, k, torch.float32)\n",
    "\n",
    "    rmem = qk_size + o_size # the results of wgmma op always in registers\n",
    "    smem = q_size + k_size + v_size # we always hold Q, K, V in smem\n",
    "\n",
    "    if MmaPV_is_RS:\n",
    "        rmem += p_size * isIntraWGOverlap\n",
    "    else:\n",
    "        smem += p_size * isIntraWGOverlap\n",
    "\n",
    "    return torch.tensor([rmem, smem])\n",
    "\n",
    "WARP_SIZE = 32\n",
    "BYTES_PER_REGISTER = 4\n",
    "WARPS_PER_WG = 4\n",
    "NUM_SUB_PARTITIONS = 4\n",
    "\n",
    "smem_limit = 256000 # 256KB\n",
    "rmem_per_sub_partition = (2**14) * BYTES_PER_REGISTER\n",
    "rmem_limit = rmem_per_sub_partition * NUM_SUB_PARTITIONS\n",
    "\n",
    "def mem_analysis(m, n, k, q_dtype = torch.int8, k_dtype = torch.int8, v_dtype = torch.float16, MmaPV_is_RS=False, isIntraWGOverlap=False):\n",
    "    mem_bytes = mem_calc(m, n, k, q_dtype, k_dtype, v_dtype, MmaPV_is_RS, isIntraWGOverlap)\n",
    "    rmem, smem = mem_bytes\n",
    "    ratio = mem_bytes / torch.tensor([rmem_limit, smem_limit])\n",
    "    print(f\"rmem/rmem_limit: {ratio[0]:.2f}, smem/smem_limit: {ratio[1]:.2f}\")\n",
    "    num_wg = m // 64\n",
    "    registers_per_mma_thread = rmem / (WARP_SIZE * BYTES_PER_REGISTER * WARPS_PER_WG * num_wg)\n",
    "    print(f\"registers needed to be allocated for each mma thread: {int(registers_per_mma_thread)}\")\n",
    "\n",
    "    remaining_registers = (rmem_per_sub_partition - (rmem / NUM_SUB_PARTITIONS)) / BYTES_PER_REGISTER\n",
    "    print(f\"remaining registers per sub-partition: {int(remaining_registers)}\")\n",
    "    remaining_smem = smem_limit - smem\n",
    "    print(f\"remaining smem in bytes: {int(remaining_smem)}\")\n",
    "\n",
    "# mem_analysis(192, 128, 128, torch.int8, torch.int8, torch.float16, MmaPV_is_RS=False, isIntraWGOverlap=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39804bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "limits = torch.tensor([rmem_limit, smem_limit])\n",
    "ratio_limit = torch.tensor([0.9, 1]) # it's 0.9 because we need tiny amount of extra registers for non mma operations\n",
    "\n",
    "def config_score(m, n, k, MmaPV_is_RS=False, isIntraWGOverlap=False):\n",
    "    mem_bytes = mem_calc(m=m, n=n, k=k, MmaPV_is_RS=MmaPV_is_RS, isIntraWGOverlap=isIntraWGOverlap)\n",
    "    rmem, smem = mem_bytes\n",
    "    ratio = mem_bytes / limits\n",
    "    if (ratio > ratio_limit).any():\n",
    "        return -torch.inf\n",
    "    \n",
    "    # rescale because rmem is more valuable than smem\n",
    "    ratio_rescaled = ratio * torch.tensor([25, 1])\n",
    "    return ratio_rescaled.norm(p=2) + (torch.tensor(1e-6) * isIntraWGOverlap) + (torch.tensor(1e-6) * MmaPV_is_RS)\n",
    "\n",
    "possible_m = [64, 128, 192]\n",
    "possible_n = 16 * torch.arange(4, 16)\n",
    "possible_MmaPV_is_RS = [False, True]\n",
    "possible_isIntraWGOverlap = [False, True]\n",
    "\n",
    "def top_k_configs(k, top_k=3):\n",
    "    configs = []\n",
    "    \n",
    "    # Iterate over all possible combinations\n",
    "    for m in possible_m:\n",
    "        for n in possible_n:\n",
    "            n_val = n.item() if isinstance(n, torch.Tensor) else n\n",
    "            for MmaPV_is_RS in possible_MmaPV_is_RS:\n",
    "                for isIntraWGOverlap in possible_isIntraWGOverlap:\n",
    "                    score = config_score(m, n_val, k, MmaPV_is_RS, isIntraWGOverlap)\n",
    "                    # Only include valid configs (score != -inf)\n",
    "                    score_val = score.item() if isinstance(score, torch.Tensor) else score\n",
    "                    configs.append({\n",
    "                        'm': m,\n",
    "                        'n': n_val,\n",
    "                        'k': k,\n",
    "                        'MmaPV_is_RS': MmaPV_is_RS,\n",
    "                        'isIntraWGOverlap': isIntraWGOverlap,\n",
    "                        'score': score_val\n",
    "                    })\n",
    "    \n",
    "    # Sort by score (lower is better)\n",
    "    configs.sort(key=lambda x: x['score'], reverse=True)\n",
    "    \n",
    "    # Return top k configs\n",
    "    return configs[:top_k]\n",
    "\n",
    "head_configs = {}\n",
    "for headdim in [64, 96, 128, 160, 192]:\n",
    "    head_configs[headdim] = top_k_configs(headdim, top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1cc371ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config:  {'m': 192, 'n': 176, 'k': 128, 'MmaPV_is_RS': False, 'isIntraWGOverlap': True, 'score': 22.283327102661133}\n",
      "rmem/rmem_limit: 0.89, smem/smem_limit: 0.89\n",
      "registers needed to be allocated for each mma thread: 152\n",
      "remaining registers per sub-partition: 1792\n",
      "remaining smem in bytes: 28672\n",
      "--------------------------------\n",
      "config:  {'m': 192, 'n': 176, 'k': 128, 'MmaPV_is_RS': True, 'isIntraWGOverlap': False, 'score': 22.274368286132812}\n",
      "rmem/rmem_limit: 0.89, smem/smem_limit: 0.62\n",
      "registers needed to be allocated for each mma thread: 152\n",
      "remaining registers per sub-partition: 1792\n",
      "remaining smem in bytes: 96256\n",
      "--------------------------------\n",
      "config:  {'m': 192, 'n': 176, 'k': 128, 'MmaPV_is_RS': False, 'isIntraWGOverlap': False, 'score': 22.27436637878418}\n",
      "rmem/rmem_limit: 0.89, smem/smem_limit: 0.62\n",
      "registers needed to be allocated for each mma thread: 152\n",
      "remaining registers per sub-partition: 1792\n",
      "remaining smem in bytes: 96256\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "for config in head_configs[128]:\n",
    "    print(\"config: \", config)\n",
    "    mem_analysis(\n",
    "        m = config['m'],\n",
    "        n = config['n'],\n",
    "        k = config['k'],\n",
    "        MmaPV_is_RS = config['MmaPV_is_RS'],\n",
    "        isIntraWGOverlap = config['isIntraWGOverlap']\n",
    "    )\n",
    "    print(\"--------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291f1816",
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_analysis(192, 128, 128, MmaPV_is_RS=False, isIntraWGOverlap=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dor-ltx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
